# examples/full_csv_extractor_demo.py
"""
–ü–æ–ª–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π CSV —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
"""

import tempfile
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os
import time

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent / "packages" / "pipeline-core" / "src"))
sys.path.insert(
    0, str(Path(__file__).parent.parent / "packages" / "components" / "extractors" / "src")
)

from pipeline_core import ExecutionContext, PipelineYAMLParser, get_registry
from pipeline_extractors.file.csv_extractor import CSVExtractor


def create_comprehensive_dataset():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

    n_records = 1000

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    data = {
        "user_id": range(1, n_records + 1),
        "name": [f"User_{i:04d}" for i in range(1, n_records + 1)],
        "email": [f"user{i}@example.com" for i in range(1, n_records + 1)],
        "age": np.random.randint(18, 80, n_records),
        "salary": np.random.normal(50000, 15000, n_records).round(2),
        "department": np.random.choice(["IT", "HR", "Finance", "Marketing", "Sales"], n_records),
        "active": np.random.choice([True, False], n_records, p=[0.8, 0.2]),
        "join_date": pd.date_range("2020-01-01", "2024-12-31", periods=n_records),
        "rating": np.random.uniform(1.0, 5.0, n_records).round(1),
        "city": np.random.choice(
            ["Moscow", "SPb", "Kazan", "Novosibirsk", "Yekaterinburg"], n_records
        ),
    }

    df = pd.DataFrame(data)

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    # –ü—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    df.loc[np.random.choice(df.index, 50, replace=False), "salary"] = np.nan
    df.loc[np.random.choice(df.index, 30, replace=False), "rating"] = np.nan

    # –ù–µ–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    df.loc[10:15, "age"] = ["unknown", "N/A", "25x", "null", "", "30"]

    return df


def demo_basic_extraction():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
    print("\nüìã –ë–ê–ó–û–í–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï")
    print("=" * 60)

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    df = create_comprehensive_dataset()
    temp_file = tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False)
    df.to_csv(temp_file.name, index=False)
    temp_file.close()

    print(f"üìÑ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª: {temp_file.name}")
    print(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(df)} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

    try:
        config = {"type": "csv-extractor", "file_path": temp_file.name, "output_format": "pandas"}

        extractor = CSVExtractor(config)
        context = ExecutionContext()

        start_time = time.time()
        result = extractor.execute(context)
        execution_time = time.time() - start_time

        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:")
        print(f"   Status: {result.status}")
        print(f"   –ó–∞–ø–∏—Å–µ–π: {result.processed_records}")
        print(f"   –í—Ä–µ–º—è: {execution_time:.3f} —Å–µ–∫")
        print(f"   –ö–æ–ª–æ–Ω–∫–∏: {len(result.metadata['columns'])}")
        print(f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {result.metadata['file_size']:,} –±–∞–π—Ç")

        if result.status.value == "success":
            print(f"\nüìã –ü–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏:")
            print(result.data.head(3).to_string(index=False))
            return temp_file.name, True

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        return temp_file.name, False

    return temp_file.name, False


def demo_filtering_and_selection(csv_file):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –≤—ã–±–æ—Ä–∞ –∫–æ–ª–æ–Ω–æ–∫"""
    print("\nüîç –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ò –í–´–ë–û–† –ö–û–õ–û–ù–û–ö")
    print("=" * 60)

    config = {
        "type": "csv-extractor",
        "file_path": csv_file,
        "select_columns": ["name", "email", "age", "salary", "department"],
        "rename_columns": {"name": "full_name", "email": "email_address"},
        "filter_condition": "age > 30 and department in ['IT', 'Finance']",
        "output_format": "pandas",
    }

    try:
        extractor = CSVExtractor(config)
        result = extractor.execute(ExecutionContext())

        print(f"‚úÖ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞:")
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {1000}")
        print(f"   –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {result.processed_records}")
        print(f"   –í—ã–±—Ä–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {result.metadata['columns']}")

        if result.status.value == "success":
            print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
            print(result.data.head().to_string(index=False))

            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞–º:")
            print(result.data["department"].value_counts().to_string())

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {e}")


def demo_data_types_and_validation(csv_file):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
    print("\nüîß –¢–ò–ü–´ –î–ê–ù–ù–´–• –ò –í–ê–õ–ò–î–ê–¶–ò–Ø")
    print("=" * 60)

    config = {
        "type": "csv-extractor",
        "file_path": csv_file,
        "column_types": {
            "user_id": "int",
            "age": "int",
            "salary": "float",
            "active": "bool",
            "join_date": "datetime",
        },
        "required_columns": ["user_id", "name", "email"],
        "min_records": 500,
        "skip_bad_lines": True,
        "error_tolerance": 0.1,
        "output_format": "pandas",
    }

    try:
        extractor = CSVExtractor(config)
        result = extractor.execute(ExecutionContext())

        print(f"‚úÖ –¢–∏–ø–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   –ó–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result.processed_records}")

        if result.status.value == "success":
            df = result.data
            print(f"\nüìä –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:")
            for col, dtype in df.dtypes.items():
                print(f"   {col}: {dtype}")

            print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫:")
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                print(df[numeric_cols].describe().round(2).to_string())

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–∏–ø–∏–∑–∞—Ü–∏–∏: {e}")


def demo_chunked_processing(csv_file):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è chunked –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    print("\n‚ö° CHUNKED –û–ë–†–ê–ë–û–¢–ö–ê")
    print("=" * 60)

    config = {
        "type": "csv-extractor",
        "file_path": csv_file,
        "chunk_size": 100,
        "filter_condition": "active == True",
        "max_rows": 500,
        "output_format": "pandas",
    }

    try:
        extractor = CSVExtractor(config)

        start_time = time.time()
        result = extractor.execute(ExecutionContext())
        execution_time = time.time() - start_time

        print(f"‚úÖ Chunked –æ–±—Ä–∞–±–æ—Ç–∫–∞:")
        print(f"   –†–∞–∑–º–µ—Ä chunk: {config['chunk_size']}")
        print(f"   –ó–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result.processed_records}")
        print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.3f} —Å–µ–∫")
        print(f"   Chunked processing: {result.metadata.get('chunked_processing', False)}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ chunked –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")


def demo_yaml_pipeline(csv_file):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ YAML pipeline"""
    print("\nüìù YAML PIPELINE")
    print("=" * 60)

    yaml_content = f"""
metadata:
  name: "comprehensive-csv-extraction"
  description: "–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ CSV —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"
  version: "1.0.0"

variables:
  DATA_FILE: "{csv_file}"
  MIN_AGE: 25
  TARGET_DEPT: "IT"

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
extract_all:
  type: "csv-extractor"
  config:
    file_path: "${{DATA_FILE}}"
    output_format: "pandas"
    required_columns: ["user_id", "name", "email"]

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ IT —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤
extract_it_staff:
  type: "csv-extractor"
  config:
    file_path: "${{DATA_FILE}}"
    select_columns: ["name", "email", "age", "salary", "department"]
    filter_condition: "age >= ${{MIN_AGE}} and department == '${{TARGET_DEPT}}'"
    column_types:
      age: "int"
      salary: "float"
    output_format: "dict"

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å chunked –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
extract_chunked:
  type: "csv-extractor"
  config:
    file_path: "${{DATA_FILE}}"
    chunk_size: 50
    max_rows: 200
    skip_bad_lines: true
    error_tolerance: 0.05
"""

    try:
        parser = PipelineYAMLParser()
        config = parser.parse_string(yaml_content)

        print(f"üìã Pipeline: {config.metadata.name}")
        print(f"üìä –°—Ç–∞–¥–∏–π: {len(config.stages)}")
        print(f"üîó –ü–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {config.get_execution_order()}")

        registry = get_registry()
        context = ExecutionContext()

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–∞–∂–¥—É—é —Å—Ç–∞–¥–∏—é
        for stage_name in ["extract_all", "extract_it_staff", "extract_chunked"]:
            if stage_name in config.stages:
                stage_config = config.stages[stage_name]

                print(f"\n‚ñ∂Ô∏è  –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ç–∞–¥–∏–∏: {stage_name}")

                component = registry.create_component(stage_config.type, stage_config.config)

                start_time = time.time()
                result = component.execute(context)
                execution_time = time.time() - start_time

                context.set_stage_result(stage_name, result)

                print(f"   Status: {result.status}")
                print(f"   Records: {result.processed_records}")
                print(f"   Time: {execution_time:.3f}s")

                if result.status.value == "success":
                    if isinstance(result.data, pd.DataFrame):
                        print(f"   Shape: {result.data.shape}")
                    elif isinstance(result.data, list):
                        print(f"   Records: {len(result.data)}")

        print(f"\nüéâ Pipeline –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ!")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–≤–æ–¥–∫—É
        summary = context.get_execution_summary()
        print(f"\nüìä –°–≤–æ–¥–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:")
        print(f"   –í—Å–µ–≥–æ —Å—Ç–∞–¥–∏–π: {summary['total_stages']}")
        print(f"   –£—Å–ø–µ—à–Ω—ã—Ö: {summary['successful_stages']}")
        print(f"   –ù–µ—É–¥–∞—á–Ω—ã—Ö: {summary['failed_stages']}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ YAML pipeline: {e}")
        import traceback

        traceback.print_exc()


def demo_error_handling():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫"""
    print("\nüö® –û–ë–†–ê–ë–û–¢–ö–ê –û–®–ò–ë–û–ö")
    print("=" * 60)

    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏
    problematic_data = """name,age,salary,department
John,25,50000,IT
Jane,thirty,invalid_salary,HR
Bob,35,60000,Finance
"Unclosed quote,40,70000,Marketing
Charlie,45,80000,Sales,Extra,Columns"""

    temp_file = tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False)
    temp_file.write(problematic_data)
    temp_file.close()

    print(f"üìÑ –°–æ–∑–¥–∞–Ω –ø—Ä–æ–±–ª–µ–º–Ω—ã–π —Ñ–∞–π–ª: {temp_file.name}")

    # –¢–µ—Å—Ç 1: –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º (–¥–æ–ª–∂–µ–Ω —É–ø–∞—Å—Ç—å)
    print(f"\nüî∏ –¢–µ—Å—Ç 1: –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º")
    config_strict = {
        "type": "csv-extractor",
        "file_path": temp_file.name,
        "skip_bad_lines": False,
        "column_types": {"age": "int", "salary": "float"},
    }

    try:
        extractor = CSVExtractor(config_strict)
        result = extractor.execute(ExecutionContext())
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {result.status}")
        if result.status.value == "failed":
            print(f"   –û—à–∏–±–∫–∞: {result.error_message}")
    except Exception as e:
        print(f"   –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")

    # –¢–µ—Å—Ç 2: –¢–æ–ª–µ—Ä–∞–Ω—Ç–Ω—ã–π —Ä–µ–∂–∏–º
    print(f"\nüî∏ –¢–µ—Å—Ç 2: –¢–æ–ª–µ—Ä–∞–Ω—Ç–Ω—ã–π —Ä–µ–∂–∏–º")
    config_tolerant = {
        "type": "csv-extractor",
        "file_path": temp_file.name,
        "skip_bad_lines": True,
        "error_tolerance": 0.5,  # –î–æ–ø—É—Å–∫–∞–µ–º 50% –æ—à–∏–±–æ–∫
        "infer_schema": False,  # –ß–∏—Ç–∞–µ–º –≤—Å–µ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏
    }

    try:
        extractor = CSVExtractor(config_tolerant)
        result = extractor.execute(ExecutionContext())
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {result.status}")
        print(f"   –ó–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result.processed_records}")

        if result.status.value == "success":
            print(f"   –î–∞–Ω–Ω—ã–µ:")
            print(result.data.head().to_string(index=False))
    except Exception as e:
        print(f"   –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")

    # –û—á–∏—Å—Ç–∫–∞
    try:
        os.unlink(temp_file.name)
    except:
        pass


def demo_performance_comparison():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    print("\n‚ö° –°–†–ê–í–ù–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
    print("=" * 60)

    # –°–æ–∑–¥–∞–µ–º –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª
    print("üìÑ –°–æ–∑–¥–∞–µ–º –±–æ–ª—å—à–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª...")
    df_large = create_comprehensive_dataset()
    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤ 5 —Ä–∞–∑
    df_large = pd.concat([df_large] * 5, ignore_index=True)

    temp_file = tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False)
    df_large.to_csv(temp_file.name, index=False)
    temp_file.close()

    print(f"üìä –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {len(df_large)} –∑–∞–ø–∏—Å–µ–π, {temp_file.name}")

    configs = [
        (
            "–û–±—ã—á–Ω–æ–µ —á—Ç–µ–Ω–∏–µ",
            {"type": "csv-extractor", "file_path": temp_file.name, "output_format": "pandas"},
        ),
        (
            "Chunked —á—Ç–µ–Ω–∏–µ (chunk=500)",
            {
                "type": "csv-extractor",
                "file_path": temp_file.name,
                "chunk_size": 500,
                "output_format": "pandas",
            },
        ),
        (
            "–° —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π",
            {
                "type": "csv-extractor",
                "file_path": temp_file.name,
                "filter_condition": "active == True and age > 25",
                "output_format": "pandas",
            },
        ),
        (
            "Chunked + —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è",
            {
                "type": "csv-extractor",
                "file_path": temp_file.name,
                "chunk_size": 500,
                "filter_condition": "active == True and age > 25",
                "output_format": "pandas",
            },
        ),
    ]

    results = []

    for test_name, config in configs:
        print(f"\nüî∏ {test_name}")

        try:
            extractor = CSVExtractor(config)

            start_time = time.time()
            result = extractor.execute(ExecutionContext())
            execution_time = time.time() - start_time

            if result.status.value == "success":
                print(f"   ‚úÖ –í—Ä–µ–º—è: {execution_time:.3f}s")
                print(f"   üìä –ó–∞–ø–∏—Å–µ–π: {result.processed_records}")
                print(f"   üíæ –ü–∞–º—è—Ç—å: chunked={config.get('chunk_size') is not None}")

                results.append(
                    {"test": test_name, "time": execution_time, "records": result.processed_records}
                )
            else:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {result.error_message}")

        except Exception as e:
            print(f"   üí• –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")

    # –°–≤–æ–¥–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if results:
        print(f"\nüìà –°–í–û–î–ö–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
        print("-" * 50)
        for r in results:
            print(f"{r['test']:<25} {r['time']:>6.3f}s {r['records']:>8} –∑–∞–ø–∏—Å–µ–π")

    # –û—á–∏—Å—Ç–∫–∞
    try:
        os.unlink(temp_file.name)
    except:
        pass


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–û–õ–ù–û–¶–ï–ù–ù–û–ì–û CSV –≠–ö–°–¢–†–ê–ö–¢–û–†–ê")
    print("=" * 80)
    print("–≠—Ç–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∫–∞–∂–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ CSV —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞:")
    print("‚Ä¢ –ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
    print("‚Ä¢ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –≤—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫")
    print("‚Ä¢ –†–∞–±–æ—Ç–∞ —Å —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö")
    print("‚Ä¢ Chunked –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤")
    print("‚Ä¢ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å YAML pipeline")
    print("‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫")
    print("‚Ä¢ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

    success_count = 0
    total_tests = 6

    try:
        # –î–µ–º–æ 1: –ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        csv_file, success = demo_basic_extraction()
        if success:
            success_count += 1

        if csv_file and os.path.exists(csv_file):
            # –î–µ–º–æ 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            try:
                demo_filtering_and_selection(csv_file)
                success_count += 1
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –¥–µ–º–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {e}")

            # –î–µ–º–æ 3: –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
            try:
                demo_data_types_and_validation(csv_file)
                success_count += 1
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –¥–µ–º–æ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö: {e}")

            # –î–µ–º–æ 4: Chunked –æ–±—Ä–∞–±–æ—Ç–∫–∞
            try:
                demo_chunked_processing(csv_file)
                success_count += 1
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –¥–µ–º–æ chunked –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")

            # –î–µ–º–æ 5: YAML pipeline
            try:
                demo_yaml_pipeline(csv_file)
                success_count += 1
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –¥–µ–º–æ YAML: {e}")

            # –û—á–∏—Å—Ç–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            try:
                os.unlink(csv_file)
            except:
                pass

        # –î–µ–º–æ 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        try:
            demo_error_handling()
            success_count += 1
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –¥–µ–º–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫: {e}")

        # –î–µ–º–æ 7: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–±–æ–Ω—É—Å)
        try:
            demo_performance_comparison()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –¥–µ–º–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}")

    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
    print(f"\n" + "=" * 80)
    print(f"üìà –ò–¢–û–ì–ò –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–ò")
    print(f"–£—Å–ø–µ—à–Ω—ã—Ö –¥–µ–º–æ: {success_count}/{total_tests}")

    if success_count == total_tests:
        print("üéâ –í—Å–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ!")
        print("üí° CSV —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ production!")
    elif success_count > total_tests // 2:
        print("‚úÖ –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ!")
        print("‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.")
    else:
        print("‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã –≤ —Ä–∞–±–æ—Ç–µ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞.")
        print("üîß –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.")

    print(f"\nüîß –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è CSV —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞:")
    print(f"   pip install pandas  # –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å")
    print(f"   pip install polars  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    print(f"   pip install aiofiles aiocsv  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è")


if __name__ == "__main__":
    main()
